{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports section\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from gensim import corpora, models, similarities\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"text text text Valentyn hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # ADD MORE PROCESSING\n",
    "\n",
    "    corpus = text.split(' ')\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0, 'nastya': 1, 'qwerty': 2, 'test': 3, 'text': 4, 'valentyn': 5}\n",
      "[[1 0 0 0 3 1]\n",
      " [2 0 0 0 0 0]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 1 1 0 0]]\n",
      "[3 1 2 2 1 2]\n",
      "[[1.         0.10338578 0.11593887 0.         0.        ]\n",
      " [0.10338578 1.         0.26589559 0.         0.        ]\n",
      " [0.11593887 0.26589559 1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         1.        ]\n",
      " [0.         0.         0.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def tfidf_example(texts):\n",
    "    corpuses = [tokenize(text) for text in texts]\n",
    "\n",
    "    vocab = sorted(set(sum(corpuses, [])))\n",
    "    vocab_dict = {k:i for i,k in enumerate(vocab)}\n",
    "\n",
    "    X_tf = np.zeros((len(corpuses), len(vocab)), dtype=int)\n",
    "\n",
    "    for i, doc in enumerate(corpuses):\n",
    "        for word in doc:\n",
    "            X_tf[i, vocab_dict[word]] += 1\n",
    "\n",
    "    print(X_tf.astype(bool).sum(axis=0))\n",
    "    idf = np.log(X_tf.shape[0]/X_tf.astype(bool).sum(axis=0))\n",
    "    X_tfidf = X_tf * idf\n",
    "\n",
    "    X_tfidf_norm = X_tfidf / np.linalg.norm(X_tfidf, axis=1)[:,None]\n",
    "    M = X_tfidf_norm @ X_tfidf_norm.T\n",
    "    print(M)\n",
    "\n",
    "    return \n",
    "\n",
    "texts = [\"text text text Valentyn hello\", \"hello hello\", \"Valentyn Nastya hello\", \"test qwerty\", \"test qwerty\"]\n",
    "tfidf_example(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. word2vec\n",
    "2. datasets parsing\n",
    "3. ml algorithms \n",
    "4. ml + fl \n",
    "5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
